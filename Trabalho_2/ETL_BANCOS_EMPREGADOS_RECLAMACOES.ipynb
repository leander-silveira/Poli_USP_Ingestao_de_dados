{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRATAMENTO DE DADOS BRUTOS VIA PYTHON PARA GERAÇÃO DE ARQUIVO CSV E POSTERIOR INGESTÃO DE DADOS NO POSTGRESQL.\n",
    "O CÓDIGO DAS CÉLULAS FORAM CRIADOS DE FORMA TOTALMENTE INDEPENDENTE UNS DOS OUTROS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\leander.silveira\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\leander.silveira\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\leander.silveira\\appdata\\roaming\\python\\python312\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\leander.silveira\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\leander.silveira\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\leander.silveira\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARA REALIZAR O ETL DA TABELA BANCOS FOI UTILIZADA A TABELA DE DADOS BRUTOS BANCOS.CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linhas movidas, duplicatas removidas e texto 'PRUDENCIAL' excluído com sucesso!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leander.silveira\\AppData\\Local\\Temp\\ipykernel_20792\\1205791932.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['NOME_SEM_PRUDENCIAL'] = df_final['Nome'].str.replace(' - PRUDENCIAL', '')\n"
     ]
    }
   ],
   "source": [
    "#CÓDIGO USADO PARA ETL TABELA BANCOS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Leitura da tabela\n",
    "df = pd.read_csv(r'C:\\\\poli\\\\Ingestão\\\\trabalho 2\\\\Dados Brutos\\\\Python\\\\Bancos.csv')\n",
    "\n",
    "# 1. Mover as linhas que contêm 'Prudencial' na coluna Nome para o topo\n",
    "df_prudencial = df[df['Nome'].str.contains('Prudencial', case=False, na=False)]\n",
    "df_outros = df[~df['Nome'].str.contains('Prudencial', case=False, na=False)]\n",
    "df_ordenado = pd.concat([df_prudencial, df_outros], ignore_index=True)\n",
    "\n",
    "# 2. Remover as primeiras ocorrências de CNPJ duplicados (mantendo a última)\n",
    "df_final = df_ordenado.drop_duplicates(subset='CNPJ', keep='last')\n",
    "\n",
    "#remove PRUDENCIAL dos demais como BTG Prudencial\n",
    "df_final['NOME_SEM_PRUDENCIAL'] = df_final['Nome'].str.replace(' - PRUDENCIAL', '')\n",
    "\n",
    "# Salva o DataFrame final em um novo arquivo CSV (ou sobrescreve o existente)\n",
    "df_final.to_csv(r'C:\\\\poli\\\\Ingestão\\\\trabalho 2\\\\Dados Brutos\\\\Python\\\\ETL_tabela_bancos.csv', index=False)\n",
    "\n",
    "print(\"Linhas movidas, duplicatas removidas e texto 'PRUDENCIAL' excluído com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARA REALIZAR O ETL DA TABELA EMPREGADOS FORAM UTILIZADAS AS TABELAS DE DADOS glassdoor_consolidado_join_match_less_v2_renomeado.csv E glassdoor_consolidado_join_match_v2_renomeado.csv. OS CSVs ESTÃO COM O FINAL _RENOMEADOS, POIS TEVE A CADIFICAÇÃO DE CARACTER CORRIGIDA PARA UTH-8 NO NOTPAD++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos lidos com sucesso.\n",
      "Ambos os arquivos têm as colunas esperadas.\n",
      "Arquivo CSV concatenado salvo como C:/poli/Ingestão/trabalho 2/Dados Brutos/Python/ETL_tabela_empregados.csv\n"
     ]
    }
   ],
   "source": [
    "#CÓDIGO USADO PARA ETL TABELA EMPREGADOS\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Definir os caminhos dos arquivos CSV\n",
    "caminho_arquivo_csv1 = 'C:/poli/Ingestão/trabalho 2/Dados Brutos/Python/glassdoor_consolidado_join_match_less_v2_renomeado.csv'\n",
    "caminho_arquivo_csv2 = 'C:/poli/Ingestão/trabalho 2/Dados Brutos/Python/glassdoor_consolidado_join_match_v2_renomeado.csv'\n",
    "caminho_arquivo_csv_resultado = 'C:/poli/Ingestão/trabalho 2/Dados Brutos/Python/ETL_tabela_empregados.csv'\n",
    "\n",
    "# Ler os arquivos CSV\n",
    "try:\n",
    "    df1 = pd.read_csv(caminho_arquivo_csv1)\n",
    "    df2 = pd.read_csv(caminho_arquivo_csv2)\n",
    "    print(\"Arquivos lidos com sucesso.\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao ler os arquivos CSV: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Verificar os nomes das colunas para garantir que correspondem\n",
    "colunas_esperadas = [\n",
    "    'employer_name', 'reviews_count', 'culture_count', 'salaries_count', \n",
    "    'benefits_count', 'employer-website', 'employer-headquarters', \n",
    "    'employer-founded', 'employer-industry', 'employer-revenue', 'url', \n",
    "    'Geral', 'Cultura e valores', 'Diversidade e inclusão', 'Qualidade de vida', \n",
    "    'Alta liderança', 'Remuneração e benefícios', 'Oportunidades de carreira', \n",
    "    'Recomendam para outras pessoas(%)', 'Perspectiva positiva da empresa(%)', \n",
    "    'CNPJ_Segmento', 'Nome', 'match_percent'\n",
    "]\n",
    "\n",
    "# Verificar se ambas as DataFrames têm as colunas esperadas\n",
    "if set(colunas_esperadas).issubset(df1.columns) and set(colunas_esperadas).issubset(df2.columns):\n",
    "    print(\"Ambos os arquivos têm as colunas esperadas.\")\n",
    "else:\n",
    "    print(\"Os arquivos CSV não têm as colunas esperadas.\")\n",
    "    print(f\"Colunas no primeiro arquivo: {df1.columns}\")\n",
    "    print(f\"Colunas no segundo arquivo: {df2.columns}\")\n",
    "    exit()\n",
    "\n",
    "# Concatenar os DataFrames\n",
    "df_concatenado = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Renomear as colunas conforme solicitado\n",
    "novos_nomes_colunas = {\n",
    "    'employer-website': 'employer_website',\n",
    "    'employer-headquarters': 'employer_headquarters',\n",
    "    'employer-founded': 'employer_founded',\n",
    "    'employer-industry': 'employer_industry',\n",
    "    'employer-revenue': 'employer_revenue',\n",
    "    'Cultura e valores': 'cultura_e_valores',\n",
    "    'Diversidade e inclusão': 'diversidade_e_inclusao',\n",
    "    'Qualidade de vida': 'qualidade_de_vida',\n",
    "    'Alta liderança': 'alta_lideranca',\n",
    "    'Remuneração e benefícios': 'remuneracao_e_beneficios',\n",
    "    'Oportunidades de carreira': 'oportunidades_de_carreira',\n",
    "    'Recomendam para outras pessoas(%)': 'recomendam_para_outras_pessoas_PCT',\n",
    "    'Perspectiva positiva da empresa(%)': 'perspectiva_positiva_da_empresa_PCT'\n",
    "}\n",
    "\n",
    "df_concatenado.rename(columns=novos_nomes_colunas, inplace=True)\n",
    "\n",
    "# Salvar o DataFrame concatenado em um novo arquivo CSV\n",
    "try:\n",
    "    df_concatenado.to_csv(caminho_arquivo_csv_resultado, index=False)\n",
    "    print(f\"Arquivo CSV concatenado salvo como {caminho_arquivo_csv_resultado}\")\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao salvar o arquivo CSV concatenado: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARA REALIZAR O ETL DA TABELA RECLAMAÇÕES FORAM UTILIZADAS AS TABELA DE DADOS BRUTOS PRESENTES EM PASTA ESPECÍFICA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo lido com sucesso: 2021_tri_01.csv\n",
      "Arquivo lido com sucesso: 2021_tri_02.csv\n",
      "Arquivo lido com sucesso: 2021_tri_03.csv\n",
      "Arquivo lido com sucesso: 2021_tri_04.csv\n",
      "Arquivo lido com sucesso: 2022_tri_01.csv\n",
      "Ocorreu um erro na leitura do arquivo 2022_tri_02_nao_ha_dados.csv: No columns to parse from file\n",
      "Arquivo lido com sucesso: 2022_tri_03.csv\n",
      "Arquivo lido com sucesso: 2022_tri_04.csv\n",
      "Colunas antes da renomeação:\n",
      "['Ano', 'Trimestre', 'Categoria', 'Tipo', 'CNPJ IF', 'Instituição financeira', 'Índice', 'Quantidade de reclamações reguladas procedentes', 'Quantidade de reclamações reguladas - outras', 'Quantidade de reclamações não reguladas', 'Quantidade total de reclamações', 'Quantidade total de clientes \\x96 CCS e SCR', 'Quantidade de clientes \\x96 CCS', 'Quantidade de clientes \\x96 SCR', 'Unnamed: 14']\n",
      "Colunas após renomeação:\n",
      "['ano', 'trimestre', 'categoria', 'tipo', 'CNPJ_IF', 'instituicao_financeira', 'indice', 'quantidade_de_reclamacoes_reguladas_procedentes', 'quantidade_de_reclamacoes_reguladas_outras', 'quantidade_de_reclamacoes_nao_reguladas', 'quantidade_total_de_reclamacoes', 'quantidade_total_de_clientes_CCS_e_SCR', 'quantidade_de_clientes_CCS', 'quantidade_de_clientes_SCR']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 14 columns):\n",
      " #   Column                                           Non-Null Count  Dtype \n",
      "---  ------                                           --------------  ----- \n",
      " 0   ano                                              918 non-null    int64 \n",
      " 1   trimestre                                        918 non-null    object\n",
      " 2   categoria                                        918 non-null    object\n",
      " 3   tipo                                             918 non-null    object\n",
      " 4   CNPJ_IF                                          918 non-null    object\n",
      " 5   instituicao_financeira                           918 non-null    object\n",
      " 6   indice                                           918 non-null    object\n",
      " 7   quantidade_de_reclamacoes_reguladas_procedentes  918 non-null    int64 \n",
      " 8   quantidade_de_reclamacoes_reguladas_outras       918 non-null    int64 \n",
      " 9   quantidade_de_reclamacoes_nao_reguladas          918 non-null    int64 \n",
      " 10  quantidade_total_de_reclamacoes                  918 non-null    int64 \n",
      " 11  quantidade_total_de_clientes_CCS_e_SCR           918 non-null    object\n",
      " 12  quantidade_de_clientes_CCS                       918 non-null    object\n",
      " 13  quantidade_de_clientes_SCR                       918 non-null    object\n",
      "dtypes: int64(5), object(9)\n",
      "memory usage: 100.5+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 918 entries, 0 to 917\n",
      "Data columns (total 14 columns):\n",
      " #   Column                                           Non-Null Count  Dtype \n",
      "---  ------                                           --------------  ----- \n",
      " 0   ano                                              918 non-null    int64 \n",
      " 1   trimestre                                        918 non-null    object\n",
      " 2   categoria                                        918 non-null    object\n",
      " 3   tipo                                             918 non-null    object\n",
      " 4   CNPJ_IF                                          918 non-null    object\n",
      " 5   instituicao_financeira                           918 non-null    object\n",
      " 6   indice                                           918 non-null    object\n",
      " 7   quantidade_de_reclamacoes_reguladas_procedentes  918 non-null    int64 \n",
      " 8   quantidade_de_reclamacoes_reguladas_outras       918 non-null    int64 \n",
      " 9   quantidade_de_reclamacoes_nao_reguladas          918 non-null    int64 \n",
      " 10  quantidade_total_de_reclamacoes                  918 non-null    int64 \n",
      " 11  quantidade_total_de_clientes_CCS_e_SCR           918 non-null    object\n",
      " 12  quantidade_de_clientes_CCS                       918 non-null    object\n",
      " 13  quantidade_de_clientes_SCR                       918 non-null    object\n",
      "dtypes: int64(5), object(9)\n",
      "memory usage: 100.5+ KB\n",
      "Arquivo CSV concatenado e renomeado salvo como C:/poli/Ingestão/trabalho 2/Dados Brutos/Python/Reclamacoes\\ETL_tabela_reclamacoes.csv\n"
     ]
    }
   ],
   "source": [
    "#CÓDIGO USADO PARA ETL TABELA BANCOS\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Defina o caminho da pasta onde os arquivos .csv estão localizados\n",
    "pasta = \"C:/poli/Ingestão/trabalho 2/Dados Brutos/Python/Reclamacoes\"\n",
    "\n",
    "# Lista para armazenar os DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Contador para garantir que apenas 8 arquivos sejam processados\n",
    "contador = 0\n",
    "max_arquivos = 8\n",
    "\n",
    "# Itera sobre todos os arquivos na pasta\n",
    "for file in os.listdir(pasta):\n",
    "    if file.endswith('.csv') and contador < max_arquivos:\n",
    "        caminho_arquivo = os.path.join(pasta, file)\n",
    "        try:\n",
    "            # Leia o arquivo CSV usando ponto e vírgula como delimitador\n",
    "            df_loop = pd.read_csv(caminho_arquivo, sep=\";\", encoding=\"latin-1\")\n",
    "            \n",
    "            # Adiciona o DataFrame à lista\n",
    "            dataframes.append(df_loop)\n",
    "            contador += 1\n",
    "            print(f\"Arquivo lido com sucesso: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f'Ocorreu um erro na leitura do arquivo {file}: {e}')\n",
    "\n",
    "# Verifica se há DataFrames para concatenar\n",
    "if dataframes:\n",
    "    try:\n",
    "        # Concatene todos os DataFrames em um único DataFrame\n",
    "        df_concatenado = pd.concat(dataframes, ignore_index=True)\n",
    "        \n",
    "        # Exibir as colunas para verificar possíveis discrepâncias\n",
    "        print(\"Colunas antes da renomeação:\")\n",
    "        print(df_concatenado.columns.tolist())\n",
    "        \n",
    "        # Definir o mapeamento de renomeação das colunas\n",
    "        colunas_novas = [\n",
    "            \"ano\"\n",
    "            ,\"trimestre\"\n",
    "            ,\"categoria\"\n",
    "            ,\"tipo\"\n",
    "            ,\"CNPJ_IF\"\n",
    "            ,\"instituicao_financeira\"\n",
    "            ,\"indice\"\n",
    "            ,\"quantidade_de_reclamacoes_reguladas_procedentes\"\n",
    "            ,\"quantidade_de_reclamacoes_reguladas_outras\"\n",
    "            ,\"quantidade_de_reclamacoes_nao_reguladas\"\n",
    "            ,\"quantidade_total_de_reclamacoes\"\n",
    "            ,\"quantidade_total_de_clientes_CCS_e_SCR\"\n",
    "            ,\"quantidade_de_clientes_CCS\"\n",
    "            ,\"quantidade_de_clientes_SCR\"\n",
    "            ,\"Unnamed: 14\"\n",
    "        ]\n",
    "\n",
    "        df_concatenado.columns = colunas_novas\n",
    "\n",
    "        # Renomear as colunas, tratando possíveis erros de renomeação\n",
    "    #    df_concatenado.rename(columns=novos_nomes_colunas, inplace=True)\n",
    "\n",
    "        df_concatenado.drop('Unnamed: 14',axis=1,inplace=True)\n",
    "\n",
    "        # Exibir as colunas após renomeação para verificação\n",
    "        print(\"Colunas após renomeação:\")\n",
    "        print(df_concatenado.columns.tolist())\n",
    "        \n",
    "\n",
    "\n",
    " #       df_concatenado['quantidade_total_de_clientes_CCS_e_SCR'] = df_concatenado['quantidade_total_de_clientes_CCS_e_SCR'].astype(int)\n",
    " #       df_concatenado['quantidade_total_de_clientes_CCS'] = df_concatenado['quantidade_total_de_clientes_CCS_e_SCR'].astype(int)\n",
    " #       df_concatenado['quantidade_total_de_clientes_SCR'] = df_concatenado['quantidade_total_de_clientes_CCS_e_SCR'].astype(int)\n",
    "\n",
    "        df_concatenado.info()\n",
    "\n",
    "        # Caminho para salvar o novo arquivo CSV concatenado\n",
    "        caminho_saida = os.path.join(pasta, \"ETL_tabela_reclamacoes.csv\")\n",
    "        \n",
    "        df_concatenado.info()\n",
    "\n",
    "        # Exporte o DataFrame concatenado para um novo arquivo CSV com cabeçalho\n",
    "        df_concatenado.to_csv(caminho_saida, index=False, sep=\";\", encoding=\"latin-1\")\n",
    "        print(f\"Arquivo CSV concatenado e renomeado salvo como {caminho_saida}\")\n",
    "    except Exception as e:\n",
    "        print(f'Ocorreu um erro na concatenação ou salvamento do arquivo: {e}')\n",
    "else:\n",
    "    print(\"Nenhum arquivo CSV foi lido.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
